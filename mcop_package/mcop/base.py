"""
M-COP v3.1 Reasoning Modes - Base Classes

This module defines the abstract base classes for the four fundamental
reasoning modes in M-COP: Causal, Structural, Selective, and Compositional.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from .mcop_types import (
    Hypothesis, Evidence, Problem, ReasoningMode,
    EpistemicState, MCOPContext
)


class BaseReasoningMode(ABC):
    """
    Abstract base class for all reasoning modes.

    Each mode implements a specific type of reasoning:
    - CAUSAL: Cause-effect chains, mechanistic reasoning
    - STRUCTURAL: Pattern recognition, architectural analysis
    - SELECTIVE: Filtering, pruning, constraint satisfaction
    - COMPOSITIONAL: Multi-step synthesis, protocol building
    """

    mode_type: ReasoningMode = ReasoningMode.CAUSAL
    mode_name: str = "Base"

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.max_hypotheses = self.config.get('max_hypotheses', 5)
        self.min_confidence = self.config.get('min_confidence', 0.3)

    @abstractmethod
    def generate_hypotheses(
        self,
        problem: Problem,
        context: MCOPContext
    ) -> List[Hypothesis]:
        """
        Generate initial seed hypotheses for a problem.

        Args:
            problem: The problem to reason about
            context: Current M-COP context with existing hypotheses

        Returns:
            List of new hypotheses generated by this mode
        """
        pass

    @abstractmethod
    def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        evidence: List[Evidence],
        context: MCOPContext
    ) -> Hypothesis:
        """
        Refine a hypothesis based on new evidence.

        Args:
            hypothesis: The hypothesis to refine
            evidence: New evidence to incorporate
            context: Current M-COP context

        Returns:
            Refined hypothesis (may be same object, modified)
        """
        pass

    @abstractmethod
    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """
        Evaluate a hypothesis and return confidence score.

        Args:
            hypothesis: The hypothesis to evaluate
            context: Current M-COP context

        Returns:
            Confidence score between 0.0 and 1.0
        """
        pass

    def should_prune(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> bool:
        """
        Determine if a hypothesis should be pruned.

        Default implementation prunes if confidence drops below threshold.
        """
        return hypothesis.confidence < self.min_confidence

    def create_hypothesis(
        self,
        content: str,
        confidence: float = 0.5,
        parent: Optional[Hypothesis] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Hypothesis:
        """
        Helper to create a new hypothesis with this mode's type.
        """
        h = Hypothesis(
            content=content,
            mode=self.mode_type,
            confidence=confidence,
            parent_id=parent.id if parent else None,
            iteration=parent.iteration + 1 if parent else 0,
            metadata=metadata or {}
        )
        return h

    def __repr__(self):
        return f"{self.mode_name}Mode(type={self.mode_type.name})"


class CausalMode(BaseReasoningMode):
    """
    Causal/Mechanistic reasoning mode.

    Focuses on cause-effect relationships, temporal sequences,
    and mechanistic explanations.

    Domain mappings:
    - Medicine: Pathophysiology chains
    - Science: Mechanism hypotheses
    - ARC: PHYSICS mode (gravity, flow, propagation)
    """

    mode_type = ReasoningMode.CAUSAL
    mode_name = "Causal"

    def generate_hypotheses(
        self,
        problem: Problem,
        context: MCOPContext
    ) -> List[Hypothesis]:
        """Generate causal chain hypotheses."""
        hypotheses = []

        # Extract causal indicators from problem
        causal_patterns = self._extract_causal_patterns(problem)

        for pattern in causal_patterns[:self.max_hypotheses]:
            h = self.create_hypothesis(
                content=f"Causal chain: {pattern['cause']} → {pattern['effect']}",
                confidence=pattern.get('initial_confidence', 0.5),
                metadata={'pattern': pattern}
            )
            hypotheses.append(h)

        return hypotheses

    def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        evidence: List[Evidence],
        context: MCOPContext
    ) -> Hypothesis:
        """Refine causal hypothesis with new evidence."""
        for e in evidence:
            hypothesis.add_evidence(e)

            # Adjust confidence based on evidence support
            if e.weight > 0.7:
                hypothesis.confidence = min(1.0, hypothesis.confidence + 0.1)
            elif e.weight < 0.3:
                hypothesis.confidence = max(0.0, hypothesis.confidence - 0.15)

        hypothesis.state = EpistemicState.GROWING
        return hypothesis

    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """Evaluate causal hypothesis strength."""
        base_score = hypothesis.confidence
        grounding_bonus = hypothesis.grounding_index * 0.3

        # Causal chains benefit from temporal evidence
        temporal_evidence = [
            e for e in hypothesis.evidence
            if 'temporal' in e.metadata.get('type', '')
        ]
        temporal_bonus = min(0.2, len(temporal_evidence) * 0.05)

        return min(1.0, base_score + grounding_bonus + temporal_bonus)

    def _extract_causal_patterns(self, problem: Problem) -> List[Dict[str, Any]]:
        """Extract potential causal patterns from problem description."""
        # Simple pattern extraction - in production, use NLP/LLM
        patterns = []

        # Look for causal keywords
        causal_keywords = ['because', 'causes', 'leads to', 'results in', 'therefore']
        description = problem.description.lower()

        for keyword in causal_keywords:
            if keyword in description:
                patterns.append({
                    'cause': 'identified_cause',
                    'effect': 'observed_effect',
                    'keyword': keyword,
                    'initial_confidence': 0.5
                })

        # Default pattern if none found
        if not patterns:
            patterns.append({
                'cause': 'unknown_mechanism',
                'effect': problem.description[:50],
                'initial_confidence': 0.3
            })

        return patterns


class StructuralMode(BaseReasoningMode):
    """
    Structural/Pattern reasoning mode.

    Focuses on patterns, architectures, and structural relationships.

    Domain mappings:
    - Medicine: Anatomical structures
    - Science: Experimental design
    - ARC: ATOMIC mode (discrete transformations)
    """

    mode_type = ReasoningMode.STRUCTURAL
    mode_name = "Structural"

    def generate_hypotheses(
        self,
        problem: Problem,
        context: MCOPContext
    ) -> List[Hypothesis]:
        """Generate structural pattern hypotheses."""
        hypotheses = []

        # Extract structural elements
        structures = self._identify_structures(problem)

        for struct in structures[:self.max_hypotheses]:
            h = self.create_hypothesis(
                content=f"Structural pattern: {struct['description']}",
                confidence=struct.get('confidence', 0.5),
                metadata={'structure': struct}
            )
            hypotheses.append(h)

        return hypotheses

    def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        evidence: List[Evidence],
        context: MCOPContext
    ) -> Hypothesis:
        """Refine structural hypothesis with evidence."""
        for e in evidence:
            hypothesis.add_evidence(e)

            # Structural hypotheses benefit from pattern consistency
            if 'pattern_match' in e.metadata:
                hypothesis.confidence = min(1.0, hypothesis.confidence + 0.12)

        hypothesis.state = EpistemicState.GROWING
        return hypothesis

    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """Evaluate structural hypothesis."""
        base_score = hypothesis.confidence
        grounding_bonus = hypothesis.grounding_index * 0.25

        # Bonus for structural consistency across evidence
        consistency_score = self._calculate_consistency(hypothesis)

        return min(1.0, base_score + grounding_bonus + consistency_score)

    def _identify_structures(self, problem: Problem) -> List[Dict[str, Any]]:
        """Identify structural patterns in problem."""
        structures = []

        # Look for structural indicators
        if problem.context:
            structures.append({
                'description': 'Context-derived structure',
                'confidence': 0.5,
                'elements': list(problem.context.keys())
            })

        if not structures:
            structures.append({
                'description': 'Implicit structure in problem',
                'confidence': 0.3
            })

        return structures

    def _calculate_consistency(self, hypothesis: Hypothesis) -> float:
        """Calculate structural consistency score."""
        if len(hypothesis.evidence) < 2:
            return 0.0

        # Simple consistency: more evidence = more consistent
        return min(0.2, len(hypothesis.evidence) * 0.04)


class SelectiveMode(BaseReasoningMode):
    """
    Selective/Filtering reasoning mode.

    Focuses on selection, pruning, and constraint satisfaction.

    Domain mappings:
    - Medicine: Differential diagnosis
    - Science: Hypothesis pruning
    - ARC: ENTROPIC mode (selection under constraints)
    """

    mode_type = ReasoningMode.SELECTIVE
    mode_name = "Selective"

    def generate_hypotheses(
        self,
        problem: Problem,
        context: MCOPContext
    ) -> List[Hypothesis]:
        """Generate selective/filtering hypotheses."""
        hypotheses = []

        # Generate constraint-based hypotheses
        constraints = problem.constraints or ['implicit_constraint']

        for i, constraint in enumerate(constraints[:self.max_hypotheses]):
            h = self.create_hypothesis(
                content=f"Selection criterion: {constraint}",
                confidence=0.5,
                metadata={'constraint': constraint, 'priority': i}
            )
            hypotheses.append(h)

        return hypotheses

    def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        evidence: List[Evidence],
        context: MCOPContext
    ) -> Hypothesis:
        """Refine selective hypothesis."""
        for e in evidence:
            hypothesis.add_evidence(e)

            # Selective mode: evidence either supports or eliminates
            if e.weight > 0.6:
                hypothesis.confidence = min(1.0, hypothesis.confidence + 0.15)
            else:
                hypothesis.confidence = max(0.0, hypothesis.confidence - 0.2)

        hypothesis.state = EpistemicState.GROWING
        return hypothesis

    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """Evaluate selective hypothesis."""
        # Selective hypotheses are binary: either satisfy constraints or don't
        base_score = hypothesis.confidence

        # Check constraint satisfaction
        constraint_score = self._check_constraints(hypothesis, context)

        return base_score * constraint_score

    def _check_constraints(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """Check how well hypothesis satisfies constraints."""
        constraints = context.problem.constraints
        if not constraints:
            return 1.0

        # Simple: assume partial satisfaction
        return 0.8


class CompositionalMode(BaseReasoningMode):
    """
    Compositional/Synthesis reasoning mode.

    Focuses on multi-step problem decomposition and synthesis.

    Domain mappings:
    - Medicine: Treatment protocols
    - Science: Research programs
    - ARC: COMPOSITIONAL mode (multi-step solutions)
    """

    mode_type = ReasoningMode.COMPOSITIONAL
    mode_name = "Compositional"

    def generate_hypotheses(
        self,
        problem: Problem,
        context: MCOPContext
    ) -> List[Hypothesis]:
        """Generate compositional/multi-step hypotheses."""
        hypotheses = []

        # Decompose problem into steps
        steps = self._decompose_problem(problem)

        for step_sequence in steps[:self.max_hypotheses]:
            h = self.create_hypothesis(
                content=f"Multi-step approach: {' → '.join(step_sequence)}",
                confidence=0.5,
                metadata={'steps': step_sequence}
            )
            hypotheses.append(h)

        return hypotheses

    def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        evidence: List[Evidence],
        context: MCOPContext
    ) -> Hypothesis:
        """Refine compositional hypothesis."""
        for e in evidence:
            hypothesis.add_evidence(e)

            # Compositional: each step validated increases confidence
            if 'step_validated' in e.metadata:
                hypothesis.confidence = min(1.0, hypothesis.confidence + 0.1)

        hypothesis.state = EpistemicState.GROWING
        return hypothesis

    def evaluate_hypothesis(
        self,
        hypothesis: Hypothesis,
        context: MCOPContext
    ) -> float:
        """Evaluate compositional hypothesis."""
        base_score = hypothesis.confidence
        grounding_bonus = hypothesis.grounding_index * 0.3

        # Bonus for step completion
        steps = hypothesis.metadata.get('steps', [])
        validated_steps = len([
            e for e in hypothesis.evidence
            if 'step_validated' in e.metadata
        ])

        completion_bonus = (validated_steps / max(1, len(steps))) * 0.2

        return min(1.0, base_score + grounding_bonus + completion_bonus)

    def _decompose_problem(self, problem: Problem) -> List[List[str]]:
        """Decompose problem into step sequences."""
        # Simple decomposition - in production, use LLM
        sequences = []

        # Default 3-step approach
        sequences.append([
            'Analyze problem structure',
            'Identify key components',
            'Synthesize solution'
        ])

        # Alternative 4-step approach
        sequences.append([
            'Define constraints',
            'Generate candidates',
            'Evaluate candidates',
            'Select optimal'
        ])

        return sequences